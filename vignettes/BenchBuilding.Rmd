---
title: "Building a Benchmark"
author: "Patrick Kimes"
date: "`r BiocStyle::doc_date()`"
package: "`r BiocStyle::pkg_ver('SummarizedBenchmark')`"
abstract: >
  benchmakr package version: `r packageVersion("SummarizedBenchmark")`
output:
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Building a benchmark with the BenchDesign framework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r echo=FALSE}

```

# Introduction

When trying to decide how to perform a data analysis in R, it can be difficult to know which tool or method is the "best" for the problem. Benchmarking the performance of tools on real and simulated data sets is a common way of learning about the relative strengths and weaknesses of each approach. However, as the number of tools and parameters being compared increases, keeping track of output and how it was generated can quickly become messy. The **BenchDesign** class and associated functions implemented in `SummarizedExperiment` provide one solution for this problem. 

```{r}
library("SummarizedBenchmark")
library("magrittr")
```

# Simple Case Study

For this example, we will use the `ttest_dat` data set included with this package. 

```{r}
data(ttest_dat)
```

The data set is a data.frame containing the results of 50 two-sample t-tests. Each of the 50 tests was performed using an independently simulated set of 20 observations drawn from a single standard Normal distribution (when `H = 0`) or two mean-shifted Normal distributions (when `H = 1`).

```{r}
head(dat)
```

To control the total number of false discoveries among a collection of tests, several approaches have been proposed and implemented to compute *adjusted p-values*. In this example, we compare four different methods:

1. Bonferroni correction (`p.adjust` w/ `method = "bonferroni"`),
2. Benjamini-Hochberg (`p.adjust` w/ `method = "BH"`),
3. Storey's FDR q-value (`qvalue::qvalue`), and
4. the recently proposed Independent Hypothesis Weighting approach (`IHW::ihw`).

## Basic Approach

First, consider how benchmarking the four methods might look without the `benchmakr` framework.  

To compare methods, each is applied to `dat` and the results are stored in separate variables. 

```{r}
adjp_bonf <- p.adjust(p = dat$pval, method = "bonferroni")

adjp_bh <- p.adjust(p = dat$pval, method = "BH")

qv <- qvalue::qvalue(p = dat$pval)
adjp_qv <- qv$qvalues

adjp_ihw <- IHW::ihw(pvalues = dat$pval,
                     covariates = IHW::groups_by_filter(dat$ind_covariate, 5),
                     alpha =  0.1)
adjp_ihw <- IHW::adj_pvalues(adjp_ihw) 
```

Since the output of each method is a vector of length 50 (the number of hypothesis tests), to keep things clean, they can be combined into a single data.frame.

```{r}
adjp <- cbind.data.frame(adjp_bonf, adjp_bh, adjp_qv, adjp_ihw)
head(adjp)
```

The data.frame of adjusted p-values can be used to evaluate each method. This can be done either by directly parsing the table, or by using a framework like `SummarizedBenchmark` or `iCOBRA`. Additionally, the data.frame can be saved as a `RDS` or `Rdata` object for future reference, reducing the need for recomputing on the original data.  

While this approach can work for smaller comparisons, it can quickly become overwhelming and unweildy as the number of methods and parameters included in the benchmark increases. Furthermore, once each method is applied and the final data.frame (`adj`) is constructed, there is no way to determine *how* each adjusted p-value was calculated. While an informative name can be used to "label" each method, this does not capture the full complexity, e.g. parameters and context, where the function was evaluated. One solution might involve manually recording function calls and parameters in a separate data.frame with the hope of maintaining synchrony with the output data.frame. However, this is notoriously prone to errors, e.g. during fast "copy and paste" operations or additions and delations of parameter combinations. An alternative (and hopefully better) solution, is to use the `BenchDesign` framework of the `SummarizedBenchmark` package.


## The BenchDesign Approach 

In the `BenchDesign` framework implemented in the `SummarizedBenchmark` package, a `BenchDesign` is first constructed with the data as the sole input. 

```{r}
b <- BenchDesign(dat)
```

Then, each method of interest is added to the `BenchDesign` using `addMethod()`. 
```{r}
b <- addMethod(b, blabel = "bonf", bfunc = p.adjust,
               p = pval, method = "bonferroni")
```

At a minimum, `addMethod()` requires specifying three parameters:

1. `b`: a `bench` object to modify,
2. `blabel`: a character name for the method, and
3. `bfunc`: the function to call.

After the three parameters are specified, any parameters needed by the method, e.g. `p.adjust`, should be passed as named variables, e.g. `p = pval, method = "bonferroni"`. Notice that `pval` **does not** need to be called as `dat$pval`.  

The process of adding methods can be written more concisely using the pipe operators of the `magrittr` package.

```{r}
b %<>% addMethod(blabel = "BH", bfunc = p.adjust,
                 p = pval, method = "BH")
```

For some methods, such as the q-value and IHW approaches above, it may be necessary to call a "post-processing" function on the primary method call. This should be specified using the `bpost` parameter. 

```{r}
b %<>% addMethod("qv", qvalue::qvalue,
                  p = pval,
                  bpost = function(x) { x$qvalues })

b %<>% addMethod("ihw", IHW::ihw,
                  pvalues = pval,
                  covariates = IHW::groups_by_filter(ind_covariate, 5),
                  alpha = 0.1,
                  bpost = IHW::adj_pvalues)
```

Now, the `BenchDesign` object contains four methods. This can be verified using the `showMethods()` function.

```{r}
showMethods(b)
```

While the bench now includes all the information necessary for performing the benchmarking study, the actual adjusted p-values have not yet been calculated. To do this, we simply need to call `buildBench()`. This will return a `SummarizedBenchmark` object with a single "assay" containing the table of adjusted p-values.

```{r}
sb <- b %>% buildBench
sb
```

```{r}
head(assay(sb, 1))
```

The function call is nicely stored in the `colData` of the same object.

```{r}
colData(sb)
```

The resulting object can now be saved to disk with the method information tied directly to the resulting output. Additionally, comparison of the adjusted p-values can now be carried out using downstream methods using the `SummarizedBenchmark` framework.

